# =============================================================================
# ollama_worker.env
# Symphony Smart Homes — Ollama Worker Configuration
# Target: 2019 iMac (Intel Core i3, 64GB RAM, macOS Sonoma/Sequoia)
# Role: LLM inference worker node for Bob the Conductor (Mac Mini M4)
#
# This file is loaded by the launchd service via EnvironmentVariables.
# After editing, restart the service:
#   launchctl unload ~/Library/LaunchAgents/com.ollama.plist
#   launchctl load ~/Library/LaunchAgents/com.ollama.plist
#
# Do NOT put secrets or API keys here — this file is committed to the repo.
# =============================================================================


# =============================================================================
# NETWORK
# =============================================================================

# Listen address for Ollama HTTP API
# 0.0.0.0 = listen on all interfaces (required for Bob to reach us over LAN)
# 127.0.0.1 = localhost only (use this if you only want local access)
OLLAMA_HOST=0.0.0.0:11434

# Ollama origins (CORS) — allow connections from anywhere on the LAN
# Replace with your subnet if you want stricter control (e.g., 192.168.1.0/24)
OLLAMA_ORIGINS=*


# =============================================================================
# PERFORMANCE
# =============================================================================

# Number of parallel inference requests to handle simultaneously
# 1 = sequential (recommended for 4-core Intel i3 to prevent resource contention)
# 2 = possible for lighter models (llama3.2:3b) but may slow down each request
OLLAMA_NUM_PARALLEL=1

# Maximum number of models to keep loaded in RAM simultaneously
# 2 = keeps bob-classifier and bob-summarizer both loaded for fast switching
# 1 = more conservative (use if RAM issues arise)
# Total RAM footprint for 2 models: ~7GB (classifier: ~2GB + summarizer: ~5GB)
OLLAMA_MAX_LOADED_MODELS=2

# How long to keep a model loaded in RAM after the last request
# 5m = 5 minutes (good balance for our batch-style usage)
# 0 = unload immediately (not recommended — reload penalty on every request)
# -1 = never unload (use if you want models always hot, and have enough RAM)
OLLAMA_KEEP_ALIVE=5m

# Flash Attention optimization (experimental, improves CPU performance)
# 1 = enabled (helps on Intel CPUs)
# 0 = disabled
OLLAMA_FLASH_ATTENTION=1

# CPU thread count (leave blank to auto-detect = use all logical cores)
# For Intel i3-8100 (4 cores, 4 threads): auto is fine
# Uncomment to override:
# OLLAMA_NUM_CPU_THREADS=4


# =============================================================================
# CONTEXT / MEMORY
# =============================================================================

# Default context window size (tokens)
# 4096 = good default for most tasks
# 2048 = more conservative (use if memory issues arise with large models)
# 8192 = extended context (only for llama3.1:8b if RAM allows)
OLLAMA_CONTEXT_LENGTH=4096

# GPU layers to offload to GPU (0 = CPU only for Intel iMac — no discrete GPU)
# This iMac has no supported GPU for Ollama — leave at 0
OLLAMA_GPU_LAYERS=0


# =============================================================================
# LOGGING
# =============================================================================

# Log level: debug | info | warn | error
# Use 'info' for production, 'debug' for troubleshooting
OLLAMA_LOG_LEVEL=info

# Log file paths (set in launchd plist, not here — see setup script)
# Stdout: ~/Library/Logs/ollama_worker.log
# Stderr: ~/Library/Logs/ollama_worker_error.log


# =============================================================================
# MODELS DIRECTORY
# =============================================================================

# Override the default model storage location
# Default: ~/.ollama/models
# Uncomment and set a different path if you want models on an external drive
# or a specific partition (e.g., if your main drive is small)
# OLLAMA_MODELS=/Volumes/DataDrive/ollama/models

# Default model storage is ~/.ollama/models
# Model sizes (approximate):
#   llama3.2:3b   ~2.0GB
#   llama3.1:8b   ~4.7GB
#   mistral:7b    ~4.1GB
#   Total: ~10.8GB


# =============================================================================
# NETWORK SECURITY NOTES
# =============================================================================
#
# This machine is a LAN-only service. Ollama does NOT have authentication.
# Access control is provided by:
#   1. Your router/firewall — Ollama port 11434 should NOT be forwarded externally
#   2. macOS Application Firewall — confirm Ollama is allowed in System Preferences
#
# If you need external access (e.g., from outside your home network), use:
#   - Tailscale (recommended) — install on both Bob and the iMac
#   - SSH tunnel: ssh -L 11434:localhost:11434 [imac_ip]
#
# DO NOT expose port 11434 to the internet without authentication.


# =============================================================================
# EXPECTED PERFORMANCE ON THIS HARDWARE
# =============================================================================
#
# Intel Core i3-8100 (4 cores, 4 threads) @ 3.6GHz
# 64GB DDR4-2666 RAM
# CPU-only inference (no GPU offload)
#
# Approximate tokens/second:
#   llama3.2:3b   : ~8-14 tokens/sec  (good for classification, routing)
#   llama3.1:8b   : ~3-6 tokens/sec   (good for summarization)
#   mistral:7b    : ~3-6 tokens/sec   (good for JSON output, structured tasks)
#   bob-classifier: ~8-12 tokens/sec  (single word output, very fast)
#   bob-summarizer: ~3-6 tokens/sec   (2-5 sentences, acceptable for batches)
#
# Document classification (bob-classifier): ~1-2 seconds per document
# Document summarization (bob-summarizer): ~30-90 seconds per document
#   (depending on document length)
#
# These speeds are suitable for background processing. Bob the Conductor
# uses Anthropic/OpenAI for interactive responses and ollama for batch work.
